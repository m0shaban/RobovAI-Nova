"""
âš¡ Nova Agent - Parallel Tool Executor
======================================

Ù†Ø¸Ø§Ù… ØªÙ†ÙÙŠØ° Ù…ØªÙˆØ§Ø²ÙŠ Ù„Ù„Ø£Ø¯ÙˆØ§Øª:
- ØªÙ†ÙÙŠØ° Ø£Ø¯ÙˆØ§Øª Ù…ØªØ¹Ø¯Ø¯Ø© ÙÙŠ ÙˆÙ‚Øª ÙˆØ§Ø­Ø¯
- Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù€ dependencies Ø¨ÙŠÙ† Ø§Ù„Ø£Ø¯ÙˆØ§Øª
- Caching Ø°ÙƒÙŠ Ù„Ù„Ù†ØªØ§Ø¦Ø¬
- Error recovery ØªÙ„Ù‚Ø§Ø¦ÙŠ
"""

import asyncio
import logging
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import defaultdict
import hashlib
import json

logger = logging.getLogger(__name__)


@dataclass
class ToolTask:
    """Ù…Ù‡Ù…Ø© Ø£Ø¯Ø§Ø© ÙˆØ§Ø­Ø¯Ø©"""
    id: str
    tool_name: str
    input_data: str
    user_id: str
    dependencies: List[str] = field(default_factory=list)
    priority: int = 1
    timeout: float = 30.0
    retry_count: int = 0
    max_retries: int = 2
    status: str = "pending"  # pending, running, completed, failed
    result: Any = None
    error: Optional[str] = None
    started_at: Optional[str] = None
    completed_at: Optional[str] = None


@dataclass 
class ExecutionBatch:
    """Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù‡Ø§Ù… Ù„Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ"""
    batch_id: str
    tasks: List[ToolTask]
    created_at: str = field(default_factory=lambda: datetime.now().isoformat())
    status: str = "pending"


class ResultCache:
    """
    ğŸ“¦ Cache Ù„Ù„Ù†ØªØ§Ø¦Ø¬
    
    ÙŠØ­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
    """
    
    def __init__(self, max_size: int = 100, ttl_seconds: int = 300):
        self.max_size = max_size
        self.ttl = timedelta(seconds=ttl_seconds)
        self._cache: Dict[str, Dict] = {}
        self._access_times: Dict[str, datetime] = {}
    
    def _make_key(self, tool_name: str, input_data: str) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙØªØ§Ø­ ÙØ±ÙŠØ¯"""
        content = f"{tool_name}:{input_data}"
        return hashlib.md5(content.encode()).hexdigest()[:16]
    
    def get(self, tool_name: str, input_data: str) -> Optional[Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†ØªÙŠØ¬Ø© Ù…Ù† Ø§Ù„Ù€ cache"""
        key = self._make_key(tool_name, input_data)
        
        if key in self._cache:
            # ÙØ­Øµ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©
            access_time = self._access_times.get(key)
            if access_time and datetime.now() - access_time < self.ttl:
                logger.debug(f"ğŸ“¦ Cache hit for {tool_name}")
                return self._cache[key]
            else:
                # Ù…Ù†ØªÙ‡ÙŠ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©
                del self._cache[key]
                del self._access_times[key]
        
        return None
    
    def set(self, tool_name: str, input_data: str, result: Any):
        """Ø­ÙØ¸ Ù†ØªÙŠØ¬Ø© ÙÙŠ Ø§Ù„Ù€ cache"""
        # ØªÙ†Ø¸ÙŠÙ Ø¥Ø°Ø§ Ø§Ù…ØªÙ„Ø£
        if len(self._cache) >= self.max_size:
            self._cleanup()
        
        key = self._make_key(tool_name, input_data)
        self._cache[key] = result
        self._access_times[key] = datetime.now()
        logger.debug(f"ğŸ“¦ Cached result for {tool_name}")
    
    def _cleanup(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©"""
        now = datetime.now()
        old_keys = [
            k for k, t in self._access_times.items()
            if now - t > self.ttl
        ]
        for key in old_keys:
            self._cache.pop(key, None)
            self._access_times.pop(key, None)
        
        # Ø¥Ø°Ø§ Ù„Ø³Ù‡ Ù…Ù…ØªÙ„Ø¦ØŒ Ø§Ø­Ø°Ù Ø§Ù„Ø£Ù‚Ø¯Ù…
        if len(self._cache) >= self.max_size:
            sorted_keys = sorted(
                self._access_times.keys(),
                key=lambda k: self._access_times[k]
            )
            for key in sorted_keys[:len(self._cache) - self.max_size + 10]:
                self._cache.pop(key, None)
                self._access_times.pop(key, None)
    
    def clear(self):
        """Ù…Ø³Ø­ Ø§Ù„Ù€ cache"""
        self._cache.clear()
        self._access_times.clear()


class ParallelExecutor:
    """
    âš¡ Ø§Ù„Ù…Ù†ÙØ° Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ Ù„Ù„Ø£Ø¯ÙˆØ§Øª
    
    ÙŠÙ†ÙØ° Ø£Ø¯ÙˆØ§Øª Ù…ØªØ¹Ø¯Ø¯Ø© ÙÙŠ ÙˆÙ‚Øª ÙˆØ§Ø­Ø¯ Ù…Ø¹:
    - Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù€ dependencies
    - Caching
    - Error recovery
    - Rate limiting
    """
    
    def __init__(self, max_concurrent: int = 5):
        self.max_concurrent = max_concurrent
        self.cache = ResultCache()
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self._running_tasks: Dict[str, ToolTask] = {}
        self._results: Dict[str, Any] = {}
    
    async def execute_tool(self, task: ToolTask) -> Dict[str, Any]:
        """ØªÙ†ÙÙŠØ° Ø£Ø¯Ø§Ø© ÙˆØ§Ø­Ø¯Ø©"""
        async with self.semaphore:
            task.status = "running"
            task.started_at = datetime.now().isoformat()
            
            # ÙØ­Øµ Ø§Ù„Ù€ cache Ø£ÙˆÙ„Ø§Ù‹
            cached = self.cache.get(task.tool_name, task.input_data)
            if cached is not None:
                task.status = "completed"
                task.result = cached
                task.completed_at = datetime.now().isoformat()
                return {"success": True, "cached": True, "result": cached}
            
            try:
                # ØªÙ†ÙÙŠØ° Ø§Ù„Ø£Ø¯Ø§Ø©
                from backend.tools.registry import ToolRegistry
                
                tool_cls = ToolRegistry.get_tool(task.tool_name)
                if not tool_cls:
                    raise ValueError(f"Tool '{task.tool_name}' not found")
                
                tool = tool_cls()
                
                # ØªÙ†ÙÙŠØ° Ù…Ø¹ timeout
                result = await asyncio.wait_for(
                    tool.execute(task.input_data, task.user_id),
                    timeout=task.timeout
                )
                
                task.status = "completed"
                task.result = result
                task.completed_at = datetime.now().isoformat()
                
                # Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ù€ cache Ø¥Ø°Ø§ Ù†Ø¬Ø­
                if result.get("success", False):
                    self.cache.set(task.tool_name, task.input_data, result)
                
                return {"success": True, "result": result}
                
            except asyncio.TimeoutError:
                task.status = "failed"
                task.error = f"Timeout after {task.timeout}s"
                return {"success": False, "error": task.error}
                
            except Exception as e:
                task.status = "failed"
                task.error = str(e)
                
                # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©
                if task.retry_count < task.max_retries:
                    task.retry_count += 1
                    task.status = "pending"
                    logger.warning(f"ğŸ”„ Retrying {task.tool_name} ({task.retry_count}/{task.max_retries})")
                    return await self.execute_tool(task)
                
                return {"success": False, "error": str(e)}
    
    async def execute_batch(self, batch: ExecutionBatch) -> List[Dict[str, Any]]:
        """ØªÙ†ÙÙŠØ° Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù‡Ø§Ù… Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ"""
        batch.status = "running"
        logger.info(f"âš¡ Executing batch {batch.batch_id} with {len(batch.tasks)} tasks")
        
        # ØªÙ‚Ø³ÙŠÙ… Ø­Ø³Ø¨ Ø§Ù„Ù€ dependencies
        independent = [t for t in batch.tasks if not t.dependencies]
        dependent = [t for t in batch.tasks if t.dependencies]
        
        results = []
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø© Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ
        if independent:
            independent_results = await asyncio.gather(
                *[self.execute_tool(t) for t in independent],
                return_exceptions=True
            )
            
            for task, result in zip(independent, independent_results):
                if isinstance(result, Exception):
                    results.append({"success": False, "error": str(result), "task_id": task.id})
                else:
                    results.append({**result, "task_id": task.id})
                    self._results[task.id] = result
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ø¹ØªÙ…Ø¯Ø© Ø¨Ø§Ù„ØªØ³Ù„Ø³Ù„
        for task in dependent:
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„Ù€ dependencies
            deps_ready = all(
                self._results.get(dep_id, {}).get("success", False)
                for dep_id in task.dependencies
            )
            
            if deps_ready:
                result = await self.execute_tool(task)
                results.append({**result, "task_id": task.id})
                self._results[task.id] = result
            else:
                results.append({
                    "success": False,
                    "error": "Dependencies not met",
                    "task_id": task.id
                })
        
        batch.status = "completed"
        logger.info(f"âœ… Batch {batch.batch_id} completed")
        
        return results
    
    async def execute_parallel(self, tool_calls: List[Dict[str, Any]], user_id: str) -> List[Dict[str, Any]]:
        """
        ØªÙ†ÙÙŠØ° Ù‚Ø§Ø¦Ù…Ø© tool calls Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ
        
        Args:
            tool_calls: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ØªÙ†ÙÙŠØ°Ù‡Ø§
            user_id: Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        
        Returns:
            Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        """
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù‡Ø§Ù…
        tasks = []
        for i, tc in enumerate(tool_calls):
            task = ToolTask(
                id=f"task_{i}",
                tool_name=tc.get("tool_name") or tc.get("name"),
                input_data=tc.get("input") or tc.get("arguments", {}),
                user_id=user_id,
                dependencies=tc.get("dependencies", []),
                priority=tc.get("priority", 1)
            )
            tasks.append(task)
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©
        tasks.sort(key=lambda t: t.priority, reverse=True)
        
        # Ø¥Ù†Ø´Ø§Ø¡ batch ÙˆØªÙ†ÙÙŠØ°
        batch = ExecutionBatch(
            batch_id=f"batch_{datetime.now().strftime('%H%M%S')}",
            tasks=tasks
        )
        
        return await self.execute_batch(batch)
    
    def get_stats(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªÙ†ÙÙŠØ°"""
        return {
            "cache_size": len(self.cache._cache),
            "running_tasks": len(self._running_tasks),
            "max_concurrent": self.max_concurrent,
            "total_results": len(self._results)
        }


# ============= Integration Functions =============

_executor: Optional[ParallelExecutor] = None

def get_parallel_executor() -> ParallelExecutor:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ instance Ù…Ù† ParallelExecutor"""
    global _executor
    if _executor is None:
        _executor = ParallelExecutor()
    return _executor


async def execute_tools_parallel(
    tool_calls: List[Dict[str, Any]],
    user_id: str = "system"
) -> List[Dict[str, Any]]:
    """
    ØªÙ†ÙÙŠØ° Ø£Ø¯ÙˆØ§Øª Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ (helper function)
    
    Usage:
        results = await execute_tools_parallel([
            {"tool_name": "web_search", "input": "AI news"},
            {"tool_name": "weather", "input": "Cairo"}
        ], user_id="user123")
    """
    executor = get_parallel_executor()
    return await executor.execute_parallel(tool_calls, user_id)


async def execute_with_retry(
    tool_name: str,
    input_data: str,
    user_id: str = "system",
    max_retries: int = 2,
    timeout: float = 30.0
) -> Dict[str, Any]:
    """
    ØªÙ†ÙÙŠØ° Ø£Ø¯Ø§Ø© Ù…Ø¹ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©
    
    Usage:
        result = await execute_with_retry(
            "presentation",
            "Ø§Ø¹Ù…Ù„ Ø¨Ø±Ø²Ù†ØªÙŠØ´Ù† Ø¹Ù† Ù…ØµØ±",
            max_retries=3
        )
    """
    task = ToolTask(
        id="single_task",
        tool_name=tool_name,
        input_data=input_data,
        user_id=user_id,
        max_retries=max_retries,
        timeout=timeout
    )
    
    executor = get_parallel_executor()
    return await executor.execute_tool(task)
