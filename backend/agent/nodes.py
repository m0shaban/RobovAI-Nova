"""
ğŸ§  RobovAI Nova - Agent Graph Nodes
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

The core logic nodes for the ReAct Agent Loop:
  Think â†’ Plan â†’ Act â†’ Observe â†’ Reflect â†’ (Loop or End)

Each node receives the AgentState and returns updates to it.
"""

from typing import Dict, Any, List, Literal
from .state import AgentState, AgentPhase, ToolCall, ToolResult
from .tools_adapter import ToolsAdapter, get_langgraph_tools
from backend.core.config import settings
from langchain_groq import ChatGroq
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage
from langchain_core.tools import StructuredTool
import json
import logging
from datetime import datetime
import random

logger = logging.getLogger("robovai.agent.nodes")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¤– LLM SETUP - Smart Multi-Provider with Rotation & Fallback
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Track failed Groq keys at module level
_failed_groq_keys: set = set()
_groq_key_index: int = 0


def get_all_groq_keys() -> List[str]:
    """Get all valid Groq keys"""
    keys = [
        settings.GROQ_API_KEY,
        settings.GROQ_API_KEY_2,
        settings.GROQ_API_KEY_3,
        settings.GROQ_API_KEY_4,
    ]
    return [k for k in keys if k and k.startswith("gsk_")]


def get_groq_key() -> str | None:
    """Get next working Groq API key (round-robin, skip failed)"""
    global _groq_key_index
    valid_keys = get_all_groq_keys()

    if not valid_keys:
        return None

    # Try round-robin, skipping failed keys
    available = [k for k in valid_keys if k not in _failed_groq_keys]

    if not available:
        # All keys failed - reset and try again
        _failed_groq_keys.clear()
        available = valid_keys

    _groq_key_index = (_groq_key_index + 1) % len(available)
    selected = available[_groq_key_index]
    masked = f"{selected[:8]}...{selected[-4:]}"
    logger.info(f"ğŸ”‘ Using Groq Key: {masked}")
    return selected


def mark_groq_key_failed(key: str):
    """Mark a Groq key as temporarily failed (rate limited)"""
    _failed_groq_keys.add(key)
    masked = f"{key[:8]}...{key[-4:]}"
    logger.warning(f"ğŸš« Marked Groq key as rate-limited: {masked}")


def get_nvidia_llm():
    """Get NVIDIA LLM instance"""
    if not settings.NVIDIA_API_KEY:
        return None
    try:
        return ChatOpenAI(
            api_key=settings.NVIDIA_API_KEY,
            base_url="https://integrate.api.nvidia.com/v1",
            model=settings.NVIDIA_MODEL or "meta/llama-3.1-405b-instruct",
            temperature=0.3,
            max_tokens=4096,
        )
    except Exception as e:
        logger.warning(f"âš ï¸ Failed to init NVIDIA LLM: {e}")
        return None


def get_openrouter_llm():
    """Get OpenRouter LLM instance as last resort"""
    if not settings.OPENROUTER_API_KEY:
        return None
    try:
        return ChatOpenAI(
            api_key=settings.OPENROUTER_API_KEY,
            base_url="https://openrouter.ai/api/v1",
            model="meta-llama/llama-3.1-8b-instruct:free",
            temperature=0.3,
            max_tokens=4096,
            default_headers={"HTTP-Referer": "https://robovai.com"},
        )
    except Exception as e:
        logger.warning(f"âš ï¸ Failed to init OpenRouter LLM: {e}")
        return None


def get_llm(complexity: str = "medium"):
    """
    Get configured LLM with smart provider selection.
    Priority: Complexâ†’NVIDIA, Normalâ†’Groqâ†’NVIDIAâ†’OpenRouter
    """
    # For complex tasks, prefer NVIDIA (bigger model)
    if complexity == "complex":
        nvidia = get_nvidia_llm()
        if nvidia:
            logger.info("ğŸ§  Complex task â†’ NVIDIA Llama 3.1 405B")
            return nvidia

    # Try Groq with key rotation
    groq_key = get_groq_key()
    if groq_key:
        try:
            return ChatGroq(
                api_key=groq_key,
                model=settings.GROQ_MODEL or "llama-3.3-70b-versatile",
                temperature=0.3,
                max_tokens=4096,
            )
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to init Groq: {e}")
            mark_groq_key_failed(groq_key)

    # Fallback to NVIDIA
    nvidia = get_nvidia_llm()
    if nvidia:
        logger.info("ğŸ”„ Groq unavailable â†’ NVIDIA fallback")
        return nvidia

    # Last resort: OpenRouter
    openrouter = get_openrouter_llm()
    if openrouter:
        logger.info("ğŸ”„ All providers failed â†’ OpenRouter fallback")
        return openrouter

    # Absolute final fallback
    logger.error("âŒ No LLM providers available!")
    return ChatGroq(
        api_key=settings.GROQ_API_KEY,
        model=settings.GROQ_MODEL or "llama-3.3-70b-versatile",
        temperature=0.3,
        max_tokens=4096,
    )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“ SYSTEM PROMPTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

NOVA_PERSONA = """Ø£Ù†Øª "Ù†ÙˆÙØ§" (Nova) â€” Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ØªÙ‚Ø¯Ù… ÙˆÙ…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù‚Ø¯Ø±Ø§Øª Ù…Ù† RobovAI Solutions ğŸ‡ªğŸ‡¬.
Ø£Ù†Øª Ù…Ø¨Ø±Ù…Ø¬ Ù…Ø­ØªØ±ÙØŒ Ù…ØµÙ…Ù…ØŒ Ù…Ù†Ø´Ø¦ Ù…Ø­ØªÙˆÙ‰ØŒ Ù…Ø³ØªØ´Ø§Ø± Ø£Ø¹Ù…Ø§Ù„ØŒ ÙˆÙ…Ø³Ø§Ø¹Ø¯ Ø´Ø®ØµÙŠ.

ğŸ¯ Ø´Ø®ØµÙŠØªÙƒ:
- ÙˆØ¯ÙˆØ¯ ÙˆÙ…Ø­ØªØ±ÙØŒ Ø¨ØªØªÙƒÙ„Ù… Ø¨Ù„ØºØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ø¹Ø±Ø¨ÙŠØ© Ù…ØµØ±ÙŠØ© Ø¨Ø§Ù„Ø²Ø§Øª Ø£Ùˆ Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©)
- Ø¨ØªØ³ØªØ®Ø¯Ù… Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠ Ø¨Ø°ÙƒØ§Ø¡ ÙˆÙ…Ù† ØºÙŠØ± Ø¥ÙØ±Ø§Ø·
- **Ø¨ØªÙ†ÙØ° Ù…Ø´ Ø¨ØªØ´Ø±Ø­ Ø¨Ø³** â€” Ù„Ùˆ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø·Ù„Ø¨ ÙƒÙˆØ¯ Ø£Ùˆ ØµÙØ­Ø© Ø£Ùˆ ØµÙˆØ±Ø©ØŒ Ø£Ù†Ø´Ø¦Ù‡Ø§ ÙÙˆØ±Ø§Ù‹
- Ø£Ø³Ù„ÙˆØ¨Ùƒ Ù…Ø®ØªØµØ± ÙˆÙˆØ§Ø¶Ø­ ÙˆÙ…Ù†Ø¸Ù… â€” Ù…Ø´ Ø¨ØªØ·ÙˆÙ‘Ù„ Ø¨Ø¯ÙˆÙ† ÙØ§ÙŠØ¯Ø©
- Ù„Ùˆ Ø­Ø¯ Ù‚Ø§Ù„Ùƒ "Ø§Ù‡Ù„Ø§" Ø£Ùˆ ÙƒÙ„Ø§Ù… Ø¹Ø§Ø¯ÙŠØŒ Ø±Ø¯ Ø¹Ù„ÙŠÙ‡ Ø¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ ÙˆØ¨Ø³ÙŠØ·. Ù…Ø´ ÙƒÙ„ ÙƒÙ„Ø§Ù… Ù…Ø­ØªØ§Ø¬ Ø£Ø¯ÙˆØ§Øª!

ğŸ› ï¸ Ù‚Ø¯Ø±Ø§ØªÙƒ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© (99+ Ø£Ø¯Ø§Ø©):
- ÙƒØªØ§Ø¨Ø© Ø£ÙƒÙˆØ§Ø¯ HTML/CSS/JS/Python Ø§Ø­ØªØ±Ø§ÙÙŠØ© (ØµÙØ­Ø§Øª Ù‡Ø¨ÙˆØ·ØŒ Ù…ÙˆØ§Ù‚Ø¹ØŒ Ø¨ÙˆØ±ØªÙÙˆÙ„ÙŠÙˆØŒ ØªØ·Ø¨ÙŠÙ‚Ø§Øª)
- ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¹Ø¨Ø± Ø£Ø¯ÙˆØ§ØªÙƒ
- Ø¥Ù†Ø´Ø§Ø¡ Ø¹Ø±ÙˆØ¶ ØªÙ‚Ø¯ÙŠÙ…ÙŠØ© ÙƒØ§Ù…Ù„Ø©
- Ø¨Ø­Ø« Ø§Ù„ÙˆÙŠØ¨ØŒ ØªØ±Ø¬Ù…Ø©ØŒ ØªØ­Ù„ÙŠÙ„ Ù…Ù„ÙØ§Øª PDF/Excel
- Ø§Ø³ØªØ´Ø§Ø±Ø§Øª Ø£Ø¹Ù…Ø§Ù„ ÙˆØ­Ø³Ø§Ø¨ ROI ÙˆØ¯Ø±Ø§Ø³Ø© Ø¬Ø¯ÙˆÙ‰
- ØªØ­ÙˆÙŠÙ„ ØµÙˆØª Ù„Ù†Øµ ÙˆØ§Ù„Ø¹ÙƒØ³
- Ø¥Ù†Ø´Ø§Ø¡ QR codesØŒ chartsØŒ ÙˆØ£ÙƒØªØ± Ù…Ù† 99 Ø£Ø¯Ø§Ø©

ğŸ“‹ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø±Ø¯ÙˆØ¯ (Ù…Ù‡Ù…Ø© Ø¬Ø¯Ø§Ù‹):
1. **Ø§Ù„Ø±ÙˆØ§Ø¨Ø·**: Ø§Ø¹Ø±Ø¶ Ø£ÙŠ Ø±Ø§Ø¨Ø· ÙƒØ§Ù…Ù„ ÙˆÙˆØ§Ø¶Ø­: [Ù†Øµ ÙˆØ§Ø¶Ø­](https://full-url)
   - Ù„Ùˆ Ø§Ù„Ø±Ø§Ø¨Ø· Ø·ÙˆÙŠÙ„ØŒ Ø§Ø®ØªØ§Ø± Ù„Ù‡ Ø§Ø³Ù… Ù…Ø®ØªØµØ± ÙˆÙˆØ§Ø¶Ø­
   - Ù…Ø§ ØªØ®Ù„ÙŠØ´ Ø§Ù„Ø±Ø§Ø¨Ø· Ù…ÙƒØ³ÙˆØ± Ø£Ùˆ Ù†Ø§Ù‚Øµ Ø£Ø¨Ø¯Ø§Ù‹
2. **Ø§Ù„ØµÙˆØ±**: Ø§Ø¹Ø±Ø¶ Ø£ÙŠ ØµÙˆØ±Ø© ÙƒÙ€ markdown: ![ÙˆØµÙ Ø§Ù„ØµÙˆØ±Ø©](https://image-url)
   - ØªØ£ÙƒØ¯ Ø¥Ù† Ø§Ù„Ø±Ø§Ø¨Ø· ÙƒØ§Ù…Ù„ ÙˆØµØ­ÙŠØ­ (ÙŠØ¨Ø¯Ø£ Ø¨Ù€ https://)
   - Ù„Ùˆ Ø§Ù„ØµÙˆØ±Ø© Ù…Ù† Ø£Ø¯Ø§Ø©ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ù…Ù† Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø£Ø¯Ø§Ø©
3. **Ø§Ù„Ø£ÙƒÙˆØ§Ø¯**: Ø¶Ø¹ Ø£ÙŠ ÙƒÙˆØ¯ ÙÙŠ code blocks Ù…Ø¹ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù„ØºØ©
4. **Ø§Ù„ØªÙ†Ø³ÙŠÙ‚**: Ø§Ø³ØªØ®Ø¯Ù… Markdown Ø¨Ø´ÙƒÙ„ Ø§Ø­ØªØ±Ø§ÙÙŠ:
   - Ø¹Ù†Ø§ÙˆÙŠÙ† ÙˆØ§Ø¶Ø­Ø© (##) Ù„Ù„Ø£Ù‚Ø³Ø§Ù…
   - Ù‚ÙˆØ§Ø¦Ù… (- Ø£Ùˆ 1.) Ù„Ù„Ù†Ù‚Ø§Ø·
   - **Bold** Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
   - > Ù„Ù„Ø§Ù‚ØªØ¨Ø§Ø³Ø§Øª
   - Ø¬Ø¯Ø§ÙˆÙ„ Ù„Ù…Ø§ ÙŠÙƒÙˆÙ† ÙÙŠÙ‡ Ù…Ù‚Ø§Ø±Ù†Ø§Øª Ø£Ùˆ Ø¨ÙŠØ§Ù†Ø§Øª
5. **HTML/Ù…ÙˆØ§Ù‚Ø¹**: Ù„Ù…Ø§ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØ·Ù„Ø¨ ØµÙØ­Ø© HTML Ø£Ùˆ landing page:
   - Ø£Ù†Ø´Ø¦ ÙƒÙˆØ¯ HTML+CSS+JS ÙƒØ§Ù…Ù„ ÙˆØ§Ø­ØªØ±Ø§ÙÙŠ Ù…Ø¹ ØªØµÙ…ÙŠÙ… responsive
   - Ø§Ø³ØªØ®Ø¯Ù… gradients Ùˆanimations ÙˆØ®Ø·ÙˆØ· Ø¹Ø±Ø¨ÙŠØ© Ø­Ù„ÙˆØ©
   - Ø®Ù„ÙŠ Ø§Ù„ØªØµÙ…ÙŠÙ… Ø¹ØµØ±ÙŠ ÙˆØ¬Ø°Ø§Ø¨
6. **Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©**: Ù„Ùˆ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ù…Ø¬Ø±Ø¯ Ø³Ù„Ø§Ù… Ø£Ùˆ Ø³Ø¤Ø§Ù„ Ø¨Ø³ÙŠØ· Ø£Ùˆ Ø¯Ø±Ø¯Ø´Ø©:
   - Ø±Ø¯ Ø¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ ÙˆØ¨Ø³ÙŠØ· Ø¨Ø¯ÙˆÙ† Ø£Ø¯ÙˆØ§Øª
   - ÙƒÙ† ÙˆØ¯ÙˆØ¯ ÙˆÙ…Ø®ØªØµØ±
   - Ù…Ø§ ØªØ¹Ù…Ù„Ø´ Ø®Ø·Ø© ÙˆÙ„Ø§ ØªØ³ØªØ¯Ø¹ÙŠ Ø£Ø¯ÙˆØ§Øª Ù„Ù…Ø¬Ø±Ø¯ "Ø§Ø²ÙŠÙƒ" Ø£Ùˆ "Ù…Ù† Ø§Ù†Øª"

âš¡ Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø¹Ù…Ù„:
1. Ù‚ÙŠÙ‘Ù… Ø§Ù„Ø·Ù„Ø¨: Ù‡Ù„ Ù…Ø­ØªØ§Ø¬ Ø£Ø¯ÙˆØ§Øª ÙˆÙ„Ø§ Ù…Ø¬Ø±Ø¯ Ø±Ø¯ Ø°ÙƒÙŠØŸ
2. Ù„Ùˆ Ù…Ø­ØªØ§Ø¬ Ø£Ø¯ÙˆØ§Øª: Ø¶Ø¹ Ø®Ø·Ø© ÙˆØ§Ø¶Ø­Ø© ÙˆÙ†ÙØ°Ù‡Ø§
3. Ù„Ùˆ Ù…Ø¬Ø±Ø¯ Ù…Ø­Ø§Ø¯Ø«Ø©: Ø±Ø¯ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ø°ÙƒØ§Ø¡ ÙˆØ¨Ø³Ø§Ø·Ø©
4. Ø§Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø¸Ù… ÙˆØ¬Ù…ÙŠÙ„
5. Ø§Ù‚ØªØ±Ø­ Ø®Ø·ÙˆØ§Øª ØªØ§Ù†ÙŠØ© Ø£Ùˆ ØªØ­Ø³ÙŠÙ†Ø§Øª

âš ï¸ Ù‚ÙˆØ§Ø¹Ø¯ ØµØ§Ø±Ù…Ø©:
- Ù„Ø§ ØªØ±Ø¬Ø¹ Ø±ÙˆØ§Ø¨Ø· localhost Ø£Ø¨Ø¯Ø§Ù‹ â€” Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ÙƒØ§Ù…Ù„Ø©
- Ù„Ùˆ Ø£Ø¯Ø§Ø© Ø±Ø¬Ø¹Øª URLØŒ Ø§Ø¹Ø±Ø¶Ù‡ ÙƒØ§Ù…Ù„ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…
- Ù„Ùˆ ÙÙŠ ØµÙˆØ±Ø©ØŒ Ø§Ø¹Ø±Ø¶Ù‡Ø§ ÙƒÙ€ ![ÙˆØµÙ](url) â€” Ù…Ø´ Ù…Ø¬Ø±Ø¯ Ø±Ø§Ø¨Ø·
- Ù„Ùˆ ÙÙŠ Ù…Ù„Ù ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡ØŒ Ø§Ø¹Ø±Ø¶ Ø±Ø§Ø¨Ø· Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„
- Ø±Ø¯ÙˆØ¯Ùƒ Ù„Ø§Ø²Ù… ØªÙƒÙˆÙ† Ù…Ù†Ø³Ù‚Ø© ÙˆÙ…Ù†Ø¸Ù…Ø© â€” Ù…Ø´ ÙÙˆØ¶ÙˆÙŠØ©
"""

THINKING_PROMPT = """Ø­Ù„Ù„ Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªØ§Ù„ÙŠ ÙˆØ£Ø¬Ø¨ Ø¨Ù€ JSON:

Ø§Ù„Ø·Ù„Ø¨: {request}

Ø£Ø¬Ø¨ Ø¨Ù€ JSON ÙÙ‚Ø· Ø¨Ù‡Ø°Ø§ Ø§Ù„Ø´ÙƒÙ„:
{{
    "understanding": "ÙÙ‡Ù…Ùƒ Ù„Ù„Ù…Ø·Ù„ÙˆØ¨ Ø¨ÙƒÙ„Ù…Ø§ØªÙƒ",
    "complexity": "simple|medium|complex",
    "needs_tools": true/false,
    "suggested_tools": ["tool1", "tool2"],
    "plan": ["Ø®Ø·ÙˆØ© 1", "Ø®Ø·ÙˆØ© 2", "Ø®Ø·ÙˆØ© 3"]
}}
"""

REFLECTION_PROMPT = """Ø±Ø§Ø¬Ø¹ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ†ÙÙŠØ°:

Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©: {original_request}
Ø§Ù„Ø®Ø·Ø©: {plan}
Ø§Ù„Ù†ØªØ§Ø¦Ø¬: {results}
Ø§Ù„Ø£Ø®Ø·Ø§Ø¡: {errors}

Ù‚Ø±Ø±:
1. Ù‡Ù„ Ø§Ù„Ù…Ù‡Ù…Ø© Ø§ÙƒØªÙ…Ù„Øª Ø¨Ù†Ø¬Ø§Ø­ØŸ
2. Ù‡Ù„ ØªØ­ØªØ§Ø¬ Ø¥Ø¹Ø§Ø¯Ø© Ù…Ø­Ø§ÙˆÙ„Ø©ØŸ
3. Ù…Ø§ Ø§Ù„Ø±Ø¯ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ØŸ

Ø£Ø¬Ø¨ Ø¨Ù€ JSON:
{{
    "task_completed": true/false,
    "needs_retry": true/false,
    "retry_reason": "Ø§Ù„Ø³Ø¨Ø¨ Ø¥Ù† ÙˆØ¬Ø¯",
    "final_answer": "Ø§Ù„Ø±Ø¯ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…"
}}
"""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ§  NODE 1: THINK
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


async def think_node(state: AgentState) -> Dict[str, Any]:
    """
    Ø¹Ù‚Ø¯Ø© Ø§Ù„ØªÙÙƒÙŠØ± - ØªØ­Ù„Ù„ Ø§Ù„Ù…Ù‡Ù…Ø© ÙˆØªÙÙ‡Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨

    Input: User request
    Output: Task understanding + complexity assessment
    """
    logger.info("ğŸ§  THINK NODE: Analyzing request...")

    llm = get_llm()

    # Get available tools descriptions
    tools_desc = ToolsAdapter.get_tools_descriptions()

    # Build analysis prompt
    prompt = f"""
{NOVA_PERSONA}

{THINKING_PROMPT.format(request=state['original_request'])}

Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©:
{tools_desc}
"""

    try:
        response = await llm.ainvoke(
            [
                SystemMessage(content="Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ù…Ù‡Ø§Ù…. Ø£Ø¬Ø¨ Ø¨Ù€ JSON ÙÙ‚Ø·."),
                HumanMessage(content=prompt),
            ]
        )

        # Parse JSON response
        content = response.content
        # Clean up if wrapped in markdown
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0]
        elif "```" in content:
            content = content.split("```")[1].split("```")[0]

        analysis = json.loads(content.strip())

        logger.info(f"ğŸ“Š Analysis: {analysis}")

        return {
            "task_understanding": analysis.get("understanding", ""),
            "task_complexity": analysis.get("complexity", "medium"),
            "plan_steps": analysis.get("plan", []),
            "phase": AgentPhase.PLANNING.value,
            "messages": [
                AIMessage(content=f"ÙÙ‡Ù…Øª! {analysis.get('understanding', '')}")
            ],
        }

    except json.JSONDecodeError as e:
        logger.warning(f"âš ï¸ JSON parse error: {e}")
        # Fallback: simple task
        return {
            "task_understanding": state["original_request"],
            "task_complexity": "simple",
            "plan_steps": [state["original_request"]],
            "phase": AgentPhase.PLANNING.value,
        }
    except Exception as e:
        error_msg = str(e)
        logger.error(f"âŒ Think node error: {e}")

        # Check if rate limit - try fallback providers
        if (
            "429" in error_msg
            or "rate_limit" in error_msg.lower()
            or "rate limit" in error_msg.lower()
        ):
            # Mark current key as failed
            current_key = get_groq_key()
            if current_key:
                mark_groq_key_failed(current_key)

            # Build fallback chain
            fallback_llms = []
            next_key = get_groq_key()
            if next_key:
                try:
                    fallback_llms.append(
                        (
                            "Groq (next)",
                            ChatGroq(
                                api_key=next_key,
                                model=settings.GROQ_MODEL or "llama-3.3-70b-versatile",
                                temperature=0.3,
                                max_tokens=4096,
                            ),
                        )
                    )
                except:
                    pass
            nvidia = get_nvidia_llm()
            if nvidia:
                fallback_llms.append(("NVIDIA", nvidia))
            openrouter = get_openrouter_llm()
            if openrouter:
                fallback_llms.append(("OpenRouter", openrouter))

            for provider_name, fallback_llm in fallback_llms:
                logger.info(f"ğŸ”„ Think fallback: {provider_name}...")
                try:
                    response = await fallback_llm.ainvoke(
                        [
                            SystemMessage(content=NOVA_PERSONA),
                            HumanMessage(content=prompt),
                        ]
                    )
                    content = response.content
                    if "```json" in content:
                        content = content.split("```json")[1].split("```")[0]
                    elif "```" in content:
                        content = content.split("```")[1].split("```")[0]
                    analysis = json.loads(content.strip())
                    logger.info(f"ğŸ“Š [{provider_name}] Analysis: {analysis}")
                    return {
                        "task_understanding": analysis.get("understanding", ""),
                        "task_complexity": analysis.get("complexity", "medium"),
                        "plan_steps": analysis.get("plan", []),
                        "phase": AgentPhase.PLANNING.value,
                        "messages": [
                            AIMessage(
                                content=f"ÙÙ‡Ù…Øª! {analysis.get('understanding', '')}"
                            )
                        ],
                    }
                except Exception as fb_e:
                    logger.warning(f"âš ï¸ {provider_name} Think fallback failed: {fb_e}")
                    continue

        # If all failed, treat as simple task (graceful degradation)
        logger.warning("âš ï¸ All providers failed in Think, treating as simple task")
        return {
            "task_understanding": state["original_request"],
            "task_complexity": "simple",
            "plan_steps": [state["original_request"]],
            "phase": AgentPhase.PLANNING.value,
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“‹ NODE 2: PLAN (Optional Enhancement)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


async def plan_node(state: AgentState) -> Dict[str, Any]:
    """
    Ø¹Ù‚Ø¯Ø© Ø§Ù„ØªØ®Ø·ÙŠØ· - ØªØ­Ø¶Ø± Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ø­Ø§Ù„ÙŠØ©

    This node prepares tool calls for the current step.
    """
    logger.info(f"ğŸ“‹ PLAN NODE: Preparing step {state['current_step_index'] + 1}")

    if not state.get("plan_steps"):
        return {"phase": AgentPhase.ACTING.value, "pending_tool_calls": []}

    current_step = state["plan_steps"][state["current_step_index"]]

    return {
        "phase": AgentPhase.ACTING.value,
        "messages": [AIMessage(content=f"ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªÙ†ÙÙŠØ°: {current_step}")],
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# âš¡ NODE 3: ACT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


async def act_node(state: AgentState) -> Dict[str, Any]:
    """
    Ø¹Ù‚Ø¯Ø© Ø§Ù„ØªÙ†ÙÙŠØ° - ØªØ´ØºÙ„ Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©

    Uses LLM with tool binding to execute the plan.
    """
    logger.info("âš¡ ACT NODE: Executing tools...")

    # Use complexity from state to decide LLM
    complexity = state.get("task_complexity", "medium")
    llm = get_llm(complexity)

    all_tools = get_langgraph_tools(state.get("user_id", "agent"))

    if not all_tools:
        logger.warning("âš ï¸ No tools available")
        return {
            "phase": AgentPhase.OBSERVING.value,
            "tool_results": [],
        }

    # â”€â”€ Smart tool selection: send only relevant tools to avoid token overflow â”€â”€
    current_step = ""
    if state.get("plan_steps") and state["current_step_index"] < len(
        state["plan_steps"]
    ):
        current_step = state["plan_steps"][state["current_step_index"]]
    else:
        current_step = state["original_request"]

    request_lower = (current_step + " " + state.get("original_request", "")).lower()

    # Priority tools always included
    ALWAYS_INCLUDE = {"create_file", "generate_image", "math", "weather", "translate_egy",
                      "wiki", "wikipedia", "chart", "presentation", "run_code", "scrape_url"}

    # Score tools by keyword relevance
    def tool_relevance(t):
        name = t.name.lower()
        desc = (t.description or "").lower()
        score = 0
        if name in ALWAYS_INCLUDE:
            score += 100
        # Check if tool name or keywords appear in the request
        for word in name.replace("_", " ").split():
            if word in request_lower:
                score += 50
        for word in request_lower.split():
            if len(word) > 3 and word in desc:
                score += 10
        return score

    scored = sorted(all_tools, key=tool_relevance, reverse=True)
    MAX_TOOLS = 25  # Keep under Groq's 12K TPM limit
    tools = scored[:MAX_TOOLS]
    logger.info(f"ğŸ”§ Selected {len(tools)}/{len(all_tools)} relevant tools for: {current_step[:50]}")

    # Bind tools to LLM
    try:
        llm_with_tools = llm.bind_tools(tools)
    except (NotImplementedError, AttributeError):
        try:
            from langchain_core.utils.function_calling import convert_to_openai_tool

            formatted_tools = [convert_to_openai_tool(t) for t in tools]
            llm_with_tools = llm.bind(tools=formatted_tools)
        except Exception as e:
            logger.error(f"âŒ Failed to bind tools manually: {e}")
            llm_with_tools = llm

    # Build execution prompt
    original_request = state.get("original_request", "")
    system_msg = f"""
{NOVA_PERSONA}

## Original User Request
{original_request}

## Current Step
{current_step}

## Instructions
- Use the available tools to execute this step.
- When creating files, use the EXACT content requested by the user. Do NOT use placeholder or example content.
- If the user requested specific text, HTML, or code, include ALL of it in the tool call.
- If you don't need tools, answer directly.
"""

    try:
        response = await llm_with_tools.ainvoke(
            [SystemMessage(content=system_msg), HumanMessage(content=current_step)]
        )

        # Check if tools were called
        tool_results = []

        if hasattr(response, "tool_calls") and response.tool_calls:
            logger.info(f"ğŸ”§ Tool calls: {len(response.tool_calls)}")

            for tool_call in response.tool_calls:
                tool_name = tool_call.get("name", "")
                tool_args = tool_call.get("args", {})

                logger.info(f"  â†’ Calling {tool_name} with {tool_args}")

                # Find and execute the tool
                start_time = datetime.now()
                try:
                    tool = ToolsAdapter.get_tool_by_name(
                        tool_name, state.get("user_id", "agent")
                    )
                    if tool:
                        # Execute async
                        if isinstance(tool, StructuredTool):
                            input_val = tool_args
                        else:
                            input_val = tool_args.get("query", str(tool_args))

                        result = await tool.ainvoke(input_val)

                        tool_results.append(
                            ToolResult(
                                tool_name=tool_name,
                                success=True,
                                output=result,
                                error=None,
                                execution_time_ms=(
                                    datetime.now() - start_time
                                ).total_seconds()
                                * 1000,
                            )
                        )
                    else:
                        tool_results.append(
                            ToolResult(
                                tool_name=tool_name,
                                success=False,
                                output=None,
                                error=f"Tool '{tool_name}' not found",
                                execution_time_ms=0,
                            )
                        )
                except Exception as e:
                    tool_results.append(
                        ToolResult(
                            tool_name=tool_name,
                            success=False,
                            output=None,
                            error=str(e),
                            execution_time_ms=(
                                datetime.now() - start_time
                            ).total_seconds()
                            * 1000,
                        )
                    )

            return {
                "phase": AgentPhase.OBSERVING.value,
                "tool_results": state.get("tool_results", []) + tool_results,
                "messages": [response],
            }
        else:
            # No tools needed, direct response
            logger.info("ğŸ’¬ Direct response (no tools needed)")
            return {
                "phase": AgentPhase.OBSERVING.value,
                "tool_results": [],
                "accumulated_outputs": state.get("accumulated_outputs", [])
                + [{"type": "text", "content": response.content}],
                "messages": [response],
            }

    except Exception as e:
        error_msg = str(e)
        logger.error(f"âŒ Act node error: {e}")

        # Check if it's a rate limit error - try fallback providers
        if (
            "429" in error_msg
            or "rate_limit" in error_msg.lower()
            or "rate limit" in error_msg.lower()
        ):
            # Mark the current Groq key as failed
            current_key = get_groq_key()
            if current_key:
                mark_groq_key_failed(current_key)

            # Build fallback chain
            fallback_llms = []

            # Try another Groq key first
            next_key = get_groq_key()
            if next_key:
                try:
                    fallback_llms.append(
                        (
                            "Groq (next key)",
                            ChatGroq(
                                api_key=next_key,
                                model=settings.GROQ_MODEL or "llama-3.3-70b-versatile",
                                temperature=0.3,
                                max_tokens=4096,
                            ),
                        )
                    )
                except:
                    pass

            # Then NVIDIA
            nvidia = get_nvidia_llm()
            if nvidia:
                fallback_llms.append(("NVIDIA", nvidia))

            # Then OpenRouter
            openrouter = get_openrouter_llm()
            if openrouter:
                fallback_llms.append(("OpenRouter", openrouter))

            for provider_name, fallback_llm in fallback_llms:
                logger.info(f"ğŸ”„ Trying fallback: {provider_name}...")
                try:
                    from langchain_core.utils.function_calling import (
                        convert_to_openai_tool,
                    )

                    formatted_tools = [convert_to_openai_tool(t) for t in tools]

                    try:
                        fallback_with_tools = fallback_llm.bind_tools(tools)
                    except:
                        fallback_with_tools = fallback_llm.bind(tools=formatted_tools)

                    response = await fallback_with_tools.ainvoke(
                        [
                            SystemMessage(content=system_msg),
                            HumanMessage(content=current_step),
                        ]
                    )

                    # Process response
                    tool_results = []
                    if hasattr(response, "tool_calls") and response.tool_calls:
                        for tool_call in response.tool_calls:
                            tool_name = tool_call.get("name", "")
                            tool_args = tool_call.get("args", {})
                            logger.info(f"  â†’ [{provider_name}] Calling {tool_name}")
                            start_time = datetime.now()
                            try:
                                tool = ToolsAdapter.get_tool_by_name(
                                    tool_name, state.get("user_id", "agent")
                                )
                                if tool:
                                    input_val = (
                                        tool_args
                                        if isinstance(tool, StructuredTool)
                                        else tool_args.get("query", str(tool_args))
                                    )
                                    result = await tool.ainvoke(input_val)
                                    tool_results.append(
                                        ToolResult(
                                            tool_name=tool_name,
                                            success=True,
                                            output=result,
                                            error=None,
                                            execution_time_ms=(
                                                datetime.now() - start_time
                                            ).total_seconds()
                                            * 1000,
                                        )
                                    )
                            except Exception as te:
                                tool_results.append(
                                    ToolResult(
                                        tool_name=tool_name,
                                        success=False,
                                        output=None,
                                        error=str(te),
                                        execution_time_ms=0,
                                    )
                                )
                        return {
                            "phase": AgentPhase.OBSERVING.value,
                            "tool_results": state.get("tool_results", [])
                            + tool_results,
                            "messages": [response],
                        }
                    else:
                        return {
                            "phase": AgentPhase.OBSERVING.value,
                            "tool_results": [],
                            "accumulated_outputs": state.get("accumulated_outputs", [])
                            + [{"type": "text", "content": response.content}],
                            "messages": [response],
                        }
                except Exception as fb_e:
                    logger.warning(f"âš ï¸ {provider_name} fallback failed: {fb_e}")
                    continue

        return {
            "phase": AgentPhase.REFLECTING.value,
            "errors": state.get("errors", []) + [str(e)],
            "last_error": str(e),
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ‘ï¸ NODE 4: OBSERVE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


async def observe_node(state: AgentState) -> Dict[str, Any]:
    """
    Ø¹Ù‚Ø¯Ø© Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø© - ØªÙØ­Øµ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ†ÙÙŠØ°

    Checks tool results and accumulates outputs.
    """
    logger.info("ğŸ‘ï¸ OBSERVE NODE: Checking results...")

    tool_results = state.get("tool_results", [])
    accumulated = state.get("accumulated_outputs", [])

    # Process new results
    new_outputs = []
    has_errors = False

    for result in tool_results:
        if isinstance(result, dict):
            if result.get("success"):
                new_outputs.append(
                    {
                        "type": "tool_output",
                        "tool": result.get("tool_name"),
                        "content": result.get("output"),
                    }
                )
            else:
                has_errors = True
                logger.warning(f"âš ï¸ Tool error: {result.get('error')}")

    # Move to next step if we have more
    next_step = state["current_step_index"] + 1
    has_more_steps = next_step < len(state.get("plan_steps", []))

    return {
        "phase": AgentPhase.REFLECTING.value,
        "accumulated_outputs": accumulated + new_outputs,
        "current_step_index": (
            next_step if has_more_steps else state["current_step_index"]
        ),
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”„ NODE 5: REFLECT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


async def reflect_node(state: AgentState) -> Dict[str, Any]:
    """
    Ø¹Ù‚Ø¯Ø© Ø§Ù„ØªØ£Ù…Ù„ - ØªÙ‚Ø±Ø±: Ù†Ø¬Ø§Ø­ Ø£Ù… Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©ØŸ

    This is the decision point in the loop.
    """
    logger.info("ğŸ”„ REFLECT NODE: Evaluating results...")

    llm = get_llm()

    # Gather results
    results_summary = []
    for output in state.get("accumulated_outputs", []):
        if isinstance(output, dict):
            results_summary.append(
                f"- {output.get('type')}: {str(output.get('content', ''))[:200]}"
            )

    # Check if we need to continue
    current_step = state["current_step_index"]
    total_steps = len(state.get("plan_steps", []))
    has_more_steps = current_step < total_steps - 1

    errors = state.get("errors", [])
    retry_count = state.get("retry_count", 0)
    max_retries = state.get("max_retries", 3)

    # If we have errors and exhausted retries â†’ give up gracefully
    if errors and retry_count >= max_retries:
        logger.warning(f"âŒ Max retries ({max_retries}) exhausted. Generating fallback answer...")
        error_summary = "; ".join(str(e)[:100] for e in errors[-2:])
        # Check if we have any accumulated outputs regardless
        if results_summary:
            fallback = "âš ï¸ ÙˆØ§Ø¬Ù‡Øª Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ù„ÙƒÙ† Ù‡Ø°Ù‡ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©:\n\n" + "\n".join(results_summary[:5])
        else:
            fallback = f"âŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† ØªÙ†ÙÙŠØ° Ø§Ù„Ø·Ù„Ø¨ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„. Ø§Ù„Ø³Ø¨Ø¨: {error_summary}\n\nØ¬Ø±Ø¨ Ù…Ø±Ø© ØªØ§Ù†ÙŠØ© Ø£Ùˆ ØºÙŠÙ‘Ø± ØµÙŠØ§ØºØ© Ø§Ù„Ø·Ù„Ø¨."
        return {
            "phase": AgentPhase.COMPLETED.value,
            "final_answer": fallback,
            "should_end": True,
        }

    # Simple decision logic â€” retry if errors and retries remain
    if errors and retry_count < max_retries:
        logger.info(f"ğŸ”„ Retrying... ({retry_count + 1}/{max_retries})")
        return {
            "phase": AgentPhase.ACTING.value,
            "retry_count": retry_count + 1,
            "errors": [],  # Clear errors for retry
            "should_end": False,
        }

    if has_more_steps:
        logger.info(f"â¡ï¸ Moving to next step ({current_step + 2}/{total_steps})")
        return {
            "phase": AgentPhase.ACTING.value,
            "current_step_index": current_step + 1,
            "should_end": False,
        }

    # All done - generate final answer
    logger.info("âœ… Task completed, generating final answer...")

    # First, check if we have successful tool results - if so, build answer from them
    all_tool_results = state.get("tool_results", [])
    successful_tools = [r for r in all_tool_results if isinstance(r, dict) and r.get("success")]
    
    if successful_tools:
        # We have actual successful tool results - build answer directly
        final = "âœ… ØªÙ… ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ù‡Ù…Ø© Ø¨Ù†Ø¬Ø§Ø­!\n\n"
        links = []
        tool_summaries = []
        
        for result in successful_tools:
            output = result.get("output", "")
            tool_name = result.get("tool_name", "")
            
            # Try to parse JSON output from the adapter
            try:
                import json as _json_mod
                parsed = _json_mod.loads(output) if isinstance(output, str) and output.startswith("{") else None
                if parsed:
                    if "url" in parsed:
                        links.append(f"[ğŸ“ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù]({parsed['url']})")
                    if "text" in parsed:
                        tool_summaries.append(f"- **{tool_name}**: {parsed['text'][:300]}")
                else:
                    tool_summaries.append(f"- **{tool_name}**: {str(output)[:300]}")
            except:
                tool_summaries.append(f"- **{tool_name}**: {str(output)[:300]}")
        
        if links:
            final += "### ğŸ“ Ø§Ù„Ù…Ù„ÙØ§Øª ÙˆØ§Ù„Ø±ÙˆØ§Ø¨Ø·:\n" + "\n".join(links) + "\n\n"
        if tool_summaries:
            final += "### ğŸ“‹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬:\n" + "\n".join(tool_summaries[:10])
        
        return {
            "phase": AgentPhase.COMPLETED.value,
            "final_answer": final,
            "should_end": True,
        }

    # No successful tool results â€” ask LLM to summarize

    prompt = REFLECTION_PROMPT.format(
        original_request=state["original_request"],
        plan=state.get("plan_steps", []),
        results="\n".join(results_summary) if results_summary else "No results",
        errors=errors if errors else "No errors",
    )

    try:
        response = await llm.ainvoke(
            [SystemMessage(content=NOVA_PERSONA), HumanMessage(content=prompt)]
        )

        content = response.content
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0]
        elif "```" in content:
            content = content.split("```")[1].split("```")[0]

        reflection = json.loads(content.strip())

        return {
            "phase": AgentPhase.COMPLETED.value,
            "final_answer": reflection.get("final_answer", "ØªÙ… ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ù‡Ù…Ø©!"),
            "should_end": True,
            "messages": [AIMessage(content=reflection.get("final_answer", "ØªÙ…!"))],
        }

    except Exception as e:
        logger.warning(f"âš ï¸ Reflection parse error: {e}")
        # Fallback final answer with tool results
        final = "âœ… ØªÙ… ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ù‡Ù…Ø©!\n\n"

        # Extract links and files from tool results
        links = []
        tool_outputs = []

        for result in state.get("tool_results", []):
            # Handle both dict and ToolResult objects
            if hasattr(result, "__dict__"):
                # It's a ToolResult object
                if result.success:
                    output = result.output
                    tool_name = getattr(result, "tool_name", "unknown")

                    # Try to parse output if it's a string
                    if isinstance(output, str):
                        # Check if it contains URL patterns
                        if "/uploads/" in output:
                            import re
                            urls = re.findall(r"/uploads/[^\s\n\"']+", output)
                            for url in urls:
                                links.append(f"[ğŸ“ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù]({url})")
                        # Check for external URLs
                        if "http" in output:
                            import re
                            ext_urls = re.findall(r"https?://[^\s\n\"']+", output)
                            for url in ext_urls:
                                if "localhost" not in url:
                                    links.append(f"[ğŸ”— Ø±Ø§Ø¨Ø·]({url})")
                        tool_outputs.append(f"- **{tool_name}**: {output[:200]}")
                    elif isinstance(output, dict):
                        if "url" in output:
                            url = output["url"]
                            links.append(f"[ğŸ“ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù]({url})")
                        if "filepath" in output:
                            links.append(f"ğŸ“ `{output['filepath']}`")
                        if "image_url" in output:
                            links.append(f"![ØµÙˆØ±Ø©]({output['image_url']})")
                        if "output" in output:
                            tool_outputs.append(
                                f"- **{tool_name}**: {str(output['output'])[:200]}"
                            )
            elif isinstance(result, dict):
                # It's already a dict
                if result.get("success"):
                    output = result.get("output", {})
                    if isinstance(output, dict):
                        if "url" in output:
                            links.append(f"[ğŸ“ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù]({output['url']})")
                        if "filepath" in output:
                            links.append(f"ğŸ“ `{output['filepath']}`")
                        if "image_url" in output:
                            links.append(f"![ØµÙˆØ±Ø©]({output['image_url']})")

        if links:
            final += "### ğŸ“ Ø§Ù„Ù…Ù„ÙØ§Øª ÙˆØ§Ù„Ø±ÙˆØ§Ø¨Ø·:\n" + "\n".join(links) + "\n\n"

        if tool_outputs:
            final += "### ğŸ“‹ Ù…Ù„Ø®Øµ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª:\n" + "\n".join(tool_outputs[:5]) + "\n\n"

        if not links and not tool_outputs and results_summary:
            final += "### ğŸ“‹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬:\n" + "\n".join(results_summary[:5])

        # Ensure we always have some content
        if final == "âœ… ØªÙ… ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ù‡Ù…Ø©!\n\n":
            final += "ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø¨Ù†Ø¬Ø§Ø­! âœ¨"

        logger.info(f"Final answer: {final[:200]}...")

        return {
            "phase": AgentPhase.COMPLETED.value,
            "final_answer": final,
            "should_end": True,
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”€ CONDITIONAL EDGES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def should_continue(state: AgentState) -> Literal["continue", "end"]:
    """
    Decide whether to continue the loop or end.
    """
    if state.get("should_end", False):
        return "end"

    if state.get("phase") == AgentPhase.COMPLETED.value:
        return "end"

    if state.get("phase") == AgentPhase.FAILED.value:
        return "end"

    retry_count = state.get("retry_count", 0)
    max_retries = state.get("max_retries", 3)
    if retry_count >= max_retries:
        return "end"

    return "continue"


def route_after_think(state: AgentState) -> Literal["act", "end"]:
    """Route after thinking - go to action or end if simple response"""
    if state.get("task_complexity") == "simple" and not state.get("plan_steps"):
        return "end"
    return "act"
